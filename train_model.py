import os
import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# ê²½ë¡œ ì„¤ì •
DATA_PATH = "data/train.csv"          # ğŸš¨ train.csv ìœ„ì¹˜
MODEL_DIR = "model_assets"
os.makedirs(MODEL_DIR, exist_ok=True)

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv(DATA_PATH)

# íƒ€ê²Ÿ ë¶„ë¦¬
y = df["fraud"]
X = df.drop(columns=[
    "ID", "fraud", "age_of_driver", "safty_rating", "annual_income",
    "vehicle_price", "vehicle_weight", "year", "month", "day", "claim_day_of_week"
])

# íŒŒìƒ í”¼ì²˜ ìƒì„± (ì‚­ì œëœ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì›ë³¸ dfì—ì„œ ìƒì„±)
X["age_group"] = pd.cut(df["age_of_driver"], [0, 25, 35, 45, 55, 65, float("inf")], labels=[0, 1, 2, 3, 4, 5])
X["safty_rating_group"] = pd.cut(df["safty_rating"], [0, 20, 40, 60, 80, 100], labels=[0, 1, 2, 3, 4])
X["income_group"] = pd.cut(df["annual_income"], [0, 35000, 40000, float("inf")], labels=[0, 1, 2])
X["price_group"] = pd.cut(df["vehicle_price"], [0, 20000, 40000, float("inf")], labels=[0, 1, 2])
X["weight_group"] = pd.cut(df["vehicle_weight"], [0, 10000, 20000, float("inf")], labels=[0, 1, 2])
X["liab_prct_group"] = pd.cut(df["liab_prct"], [0, 20, 40, 60, 80, 100], labels=[0, 1, 2, 3, 4])
X["claim_est_payout_group"] = pd.cut(df["claim_est_payout"], [0, 5000, 10000, 15000, float("inf")], labels=[0, 1, 2, 3])
X["age_of_vehicle_group"] = pd.cut(df["age_of_vehicle"], [0, 5, 10, 15, float("inf")], labels=[0, 1, 2, 3])

# ë¼ë²¨ ì¸ì½”ë”©
label_cols = [
    'gender', 'marital_status', 'address_change_ind', 'living_status',
    'witness_present_ind', 'age_group', 'safty_rating_group', 'income_group',
    'price_group', 'weight_group', 'liab_prct_group', 'claim_est_payout_group',
    'age_of_vehicle_group'
]
label_encoders = {}
for col in label_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    joblib.dump(le, os.path.join(MODEL_DIR, f"label_encoder_{col}.pkl"))
    label_encoders[col] = le

    # í´ë˜ìŠ¤ ëª©ë¡ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    print(f"{col} í´ë˜ìŠ¤ ëª©ë¡: {list(le.classes_)}")

# ì›í•« ì¸ì½”ë”©
X = pd.get_dummies(X, columns=["accident_site", "channel", "vehicle_category", "vehicle_color"], drop_first=False)

# ì»¬ëŸ¼ ìˆœì„œ ì €ì¥
expected_columns = X.columns.tolist()
joblib.dump(expected_columns, os.path.join(MODEL_DIR, "expected_columns.pkl"))

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
joblib.dump(scaler, os.path.join(MODEL_DIR, "scaler.pkl"))

# í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# ëª¨ë¸ êµ¬ì„±
model = Sequential([
    Dense(128, input_dim=X_train.shape[1], activation='relu'),
    Dropout(0.4),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

# í•™ìŠµ
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)

# í‰ê°€
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)
macro_f1 = f1_score(y_test, y_pred, average='macro')
print(f"Macro F1 Score: {macro_f1}")

# ëª¨ë¸ ì €ì¥
model.save(os.path.join(MODEL_DIR, "fraud_model.h5"))
